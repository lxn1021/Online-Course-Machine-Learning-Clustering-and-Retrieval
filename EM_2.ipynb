{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a diagonal covariance Gaussian mixture model to text data\n",
    "\n",
    "In a previous assignment, we explored k-means clustering for a high-dimensional Wikipedia dataset. We can also model this data with a mixture of Gaussians, though with increasing dimension we run into two important issues associated with using a full covariance matrix for each component.\n",
    " * Computational cost becomes prohibitive in high dimensions: score calculations have complexity cubic in the number of dimensions M if the Gaussian has a full covariance matrix.\n",
    " * A model with many parameters require more data: observe that a full covariance matrix for an M-dimensional Gaussian will have M(M+1)/2 parameters to fit. With the number of parameters growing roughly as the square of the dimension, it may quickly become impossible to find a sufficient amount of data to make good inferences.\n",
    " \n",
    "Both of these issues are avoided if we require the covariance matrix of each component to be diagonal, as then it has only M parameters to fit and the score computation decomposes into M univariate score calculations. Recall from the lecture that the M-step for the full covariance is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\Sigma}_k &= \\frac{1}{N_k^{soft}} \\sum_{i=1}^N r_{ik} (x_i-\\hat{\\mu}_k)(x_i - \\hat{\\mu}_k)^T\n",
    "\\end{align*}\n",
    "\n",
    "Note that this is a square matrix with M rows and M columns, and the above equation implies that the (v, w) element is computed by\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\Sigma}_{k, v, w} &= \\frac{1}{N_k^{soft}} \\sum_{i=1}^N r_{ik} (x_{iv}-\\hat{\\mu}_{kv})(x_{iw} - \\hat{\\mu}_{kw})\n",
    "\\end{align*}\n",
    "\n",
    "When we assume that this is a diagonal matrix, then non-diagonal elements are assumed to be zero and we only need to compute each of the M elements along the diagonal independently using the following equation.\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\sigma}^2_{k, v} &= \\hat{\\Sigma}_{k, v, v}  \\\\\n",
    "&= \\frac{1}{N_k^{soft}} \\sum_{i=1}^N r_{ik} (x_{iv}-\\hat{\\mu}_{kv})^2\n",
    "\\end{align*}\n",
    "\n",
    "In this section, we will use an EM implementation to fit a Gaussian mixture model with diagonal covariances to a subset of the Wikipedia dataset. The implementation uses the above equation to compute each variance term.\n",
    "\n",
    "We'll begin by importing the dataset and coming up with a useful representation for each article. After running our algorithm on the data, we will explore the output to see whether we can give a meaningful interpretation to the fitted parameters in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphlab\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import spdiags\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Wikipedia data and extract TF-IDF features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Wikipedia data and transform each of the first 5000 document into a TF-IDF representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to lxn1021@gmail.com and will expire on November 18, 2019.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: C:\\Users\\Xiaoning\\AppData\\Local\\Temp\\graphlab_server_1555989643.log.0\n"
     ]
    }
   ],
   "source": [
    "wiki = graphlab.SFrame(\"E:\\\\Machine Learning\\\\U.W\\\\Cluster and Retrieval\\\\people_wiki.gl/\").head(5000)\n",
    "wiki[\"tf_idf\"] = graphlab.text_analytics.tf_idf(wiki[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sframe_to_scipy(x, column_name):\n",
    "    # Create triples of (row_id, feature_id, count).\n",
    "    # 1. Add a row number.\n",
    "    x = x.add_row_number()\n",
    "    # 2. Stack will transform x to have a row for each unique (row, key) pair.\n",
    "    x = x.stack(column_name, [\"feature\", \"value\"])\n",
    "    \n",
    "    # Map words into integers.\n",
    "    f = graphlab.feature_engineering.OneHotEncoder(features=[\"feature\"])\n",
    "    # 1. Fit the transformer.\n",
    "    f.fit(x)\n",
    "    # 2. The transform takes \"feature\" column and adds a new column \"feature_encoding\".\n",
    "    x = f.transform(x)\n",
    "    # 3. Get the feature mapping.\n",
    "    mapping = f[\"feature_encoding\"]\n",
    "    # 4. Get the feature id to use for each key.\n",
    "    x[\"feature_id\"] = x[\"encoded_features\"].dict_keys().apply(lambda x: x[0])\n",
    "    \n",
    "    # Create numpy arrays that contain the data for the sparse matrix.\n",
    "    i = np.array(x[\"id\"])\n",
    "    j = np.array(x[\"feature_id\"])\n",
    "    v = np.array(x[\"value\"])\n",
    "    width = x[\"id\"].max() + 1\n",
    "    height = x[\"feature_id\"].max() + 1\n",
    "    \n",
    "    # Create a sparse matrix.\n",
    "    mat = csr_matrix((v, (i, j)), shape=(width, height))\n",
    "    \n",
    "    \n",
    "    return mat, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf, map_index_to_word = sframe_to_scipy(wiki, \"tf_idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5000x100282 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 881415 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">feature</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">category</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">feature</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">conflating</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">feature</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">diamono</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">feature</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">bailouts</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">feature</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">electionruss</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">feature</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">maywoods</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">feature</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">feduring</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">feature</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">spiderbait</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">feature</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">mcin</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">feature</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">sumiswald</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">feature</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">quinta</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">9</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[100282 rows x 3 columns]<br/>Note: Only the head of the SFrame is printed.<br/>You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tfeature\tstr\n",
       "\tcategory\tstr\n",
       "\tindex\tint\n",
       "\n",
       "Rows: 100282\n",
       "\n",
       "Data:\n",
       "+---------+--------------+-------+\n",
       "| feature |   category   | index |\n",
       "+---------+--------------+-------+\n",
       "| feature |  conflating  |   0   |\n",
       "| feature |   diamono    |   1   |\n",
       "| feature |   bailouts   |   2   |\n",
       "| feature | electionruss |   3   |\n",
       "| feature |   maywoods   |   4   |\n",
       "| feature |   feduring   |   5   |\n",
       "| feature |  spiderbait  |   6   |\n",
       "| feature |     mcin     |   7   |\n",
       "| feature |  sumiswald   |   8   |\n",
       "| feature |    quinta    |   9   |\n",
       "+---------+--------------+-------+\n",
       "[100282 rows x 3 columns]\n",
       "Note: Only the head of the SFrame is printed.\n",
       "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_index_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous assignment, we will normalize each document's TF-IDF vector to be a unit vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = normalize(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5000x100282 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 881415 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM in high dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EM for high-dimensional data requires some special treatment:\n",
    "* E step and M step must be vectorized as much as possible, as explicit loops are dreadfully slow in Python.\n",
    "* All operations must be cast in terms of sparse matrix operations, to take advantage of computational savings enabled by sparsity of data.\n",
    "* Initially, some words may be entirely absent from a cluster, causing the M step to produce zero mean and variance for those words.  This means any data point with one of those words will have 0 probability of being assigned to that cluster since the cluster allows for no variability (0 variance) around that count being 0 (0 mean). Since there is a small chance for those words to later appear in the cluster, we instead assign a small positive variance (~1e-10). Doing so also prevents numerical overflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing mean parameters using k-means**\n",
    "\n",
    "Recall from the lectures that EM for Gaussian mixtures is very sensitive to the choice of initial means. With a bad initial set of means, EM may produce clusters that span a large area and are mostly overlapping. To eliminate such bad outcomes, we first produce a suitable set of initial means by using the cluster centers from running k-means. That is, we first run k-means and then take the final set of means from the converged solution as the initial means in our EM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "num_clusters = 25\n",
    "\n",
    "kmeans_model = KMeans(n_clusters=num_clusters, n_init=5, max_iter=400, random_state=1, n_jobs=1)\n",
    "kmeans_model.fit(tf_idf)\n",
    "centriods = kmeans_model.cluster_centers_\n",
    "cluster_assignment = kmeans_model.labels_\n",
    "\n",
    "means = [centroid for centroid in centriods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.79942936e-04, 1.38140667e-04, 7.52519518e-05]),\n",
       " array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.23579792e-04, 1.31387094e-04, 8.53687044e-05]),\n",
       " array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.57794488e-04, 9.10226593e-05, 8.63707823e-05]),\n",
       " array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.15889678e-04, 1.00662936e-04, 9.09111720e-05]),\n",
       " array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.90336708e-04, 1.39094073e-04, 8.35359900e-05]),\n",
       " array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.34408669e-04, 8.79722226e-05, 9.68849648e-05]),\n",
       " array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.43089838e-04, 8.02849784e-05, 8.49266981e-05]),\n",
       " array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.07034151e-04, 1.40097040e-04, 9.10716403e-05]),\n",
       " array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.05112230e-04, 1.39280540e-04, 7.45475368e-05]),\n",
       " array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.76996360e-04, 1.30430940e-04, 8.16364264e-05]),\n",
       " array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.21846594e-04, 1.47403392e-04, 8.45760373e-05]),\n",
       " array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.06149890e-04, 1.55238693e-04, 9.35933016e-05]),\n",
       " array([0.00000000e+00, 0.00000000e+00, 7.69939220e-04, ...,\n",
       "        2.02029547e-04, 1.25717072e-04, 9.33672496e-05]),\n",
       " array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.07547516e-04, 8.31657974e-05, 1.07994050e-04]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.00025411, 0.00010377,\n",
       "        0.00010457]),\n",
       " array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.07576195e-04, 1.03214956e-04, 9.27617511e-05]),\n",
       " array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.22518434e-04, 1.11184683e-04, 9.47769141e-05]),\n",
       " array([2.39584239e-04, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.66383200e-04, 1.02229585e-04, 7.45341124e-05]),\n",
       " array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.91815412e-04, 1.56645470e-04, 9.46711655e-05]),\n",
       " array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.68211934e-04, 8.74442072e-05, 9.76059782e-05]),\n",
       " array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.03583915e-04, 1.13154052e-04, 9.46074606e-05]),\n",
       " array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.81351373e-04, 1.37846017e-04, 8.12510991e-05]),\n",
       " array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.27670892e-04, 1.43313323e-04, 9.40545820e-05]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.00020392, 0.00010381,\n",
       "        0.00010233]),\n",
       " array([0.00000000e+00, 2.69826164e-04, 0.00000000e+00, ...,\n",
       "        1.63292080e-04, 1.35964683e-04, 7.72546921e-05])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing cluster weights**\n",
    "\n",
    "We will initialize each cluster weight to be the proportion of documents assigned to that cluster by k-means above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = tf_idf.shape[0]\n",
    "weights = []\n",
    "\n",
    "for i in xrange(num_clusters):\n",
    "    # Compute the number of data points assigned to cluster i:\n",
    "    num_assigned = len(cluster_assignment[cluster_assignment==i])\n",
    "    w = float(num_assigned)/num_docs\n",
    "    weights.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing covariances**\n",
    "\n",
    "To initialize our covariance parameters, we compute $\\hat{\\sigma}_{k, j}^2 = \\sum_{i=1}^{N}(x_{i,j} - \\hat{\\mu}_{k, j})^2$ for each feature $j$.  For features with really tiny variances, we assign 1e-8 instead to prevent numerical instability. We do this computation in a vectorized fashion in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diag(array):\n",
    "    n = len(array)\n",
    "    \n",
    "    return spdiags(array, 0, n, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "covs = []\n",
    "\n",
    "for i in xrange(num_clusters):\n",
    "    member_rows = tf_idf[cluster_assignment==i]\n",
    "    cov = (member_rows.multiply(member_rows) - 2*member_rows.dot(diag(means[i]))).sum(axis=0).A1 / member_rows.shape[0] \\\n",
    "            + means[i]**2\n",
    "    cov[cov < 1e-8] = 1e-8\n",
    "    covs.append(cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running EM**\n",
    "\n",
    "Now that we have initialized all of our parameters, run EM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(x, axis):\n",
    "    x_max = np.max(x, axis=axis)\n",
    "    if axis == 1:\n",
    "        return x_max + np.log( np.sum(np.exp(x-x_max[:,np.newaxis]), axis=1) )\n",
    "    else:\n",
    "        return x_max + np.log( np.sum(np.exp(x-x_max), axis=0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logpdf_diagonal_gaussian(x, mean, cov):\n",
    "    n = x.shape[0]\n",
    "    dim = x.shape[1]\n",
    "    assert(dim == len(mean) and dim == len(cov))\n",
    "\n",
    "    # multiply each i-th column of x by (1/(2*sigma_i)), where sigma_i is sqrt of variance of i-th variable.\n",
    "    scaled_x = x.dot( diag(1./(2*np.sqrt(cov))) )\n",
    "    # multiply each i-th entry of mean by (1/(2*sigma_i))\n",
    "    scaled_mean = mean/(2*np.sqrt(cov))\n",
    "\n",
    "    # sum of pairwise squared Eulidean distances gives SUM[(x_i - mean_i)^2/(2*sigma_i^2)]\n",
    "    return -np.sum(np.log(np.sqrt(2*np.pi*cov))) - pairwise_distances(scaled_x, [scaled_mean], 'euclidean').flatten()**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_for_high_dimension(data, means, covs, weights, cov_smoothing=1e-5, maxiter=int(1e3), thresh=1e-4, verbose=False):\n",
    "    n = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "    mu = deepcopy(means)\n",
    "    sigma = deepcopy(covs)\n",
    "    K = len(mu)\n",
    "    weights = np.array(weights)\n",
    "    \n",
    "    l1 = None\n",
    "    l1_trace = []\n",
    "    \n",
    "    for i in range(maxiter):\n",
    "        # E-step: compute responsibilities\n",
    "        logresp = np.zeros((n, K))    \n",
    "        for k in xrange(K):\n",
    "            logresp[:, k] = np.log(weights[k]) + logpdf_diagonal_gaussian(data, mu[k], sigma[k])\n",
    "        l1_new = np.sum(log_sum_exp(logresp, axis=1))\n",
    "        if verbose:\n",
    "            print(l1_new)\n",
    "        sys.stdout.flush()\n",
    "        logresp -= np.vstack(log_sum_exp(logresp, axis=1))\n",
    "        resp = np.exp(logresp)\n",
    "        counts = np.sum(resp, axis=0)\n",
    "        \n",
    "        # M-step: update weights, means, covariances\n",
    "        weights = counts/np.sum(counts)\n",
    "        for k in range(K):\n",
    "            mu[k] = (diag(resp[:, k]).dot(data)).sum(axis=0)/counts[k]\n",
    "            mu[k] = mu[k].A1\n",
    "            sigma[k] = diag(resp[:, k]).dot(data.multiply(data)-2*data.dot(diag(mu[k]))).sum(axis=0) \\\n",
    "                        + (mu[k]**2)*counts[k]\n",
    "            sigma[k] = sigma[k].A1 / counts[k] + cov_smoothing*np.ones(dim)\n",
    "        \n",
    "        # Check for convergence in log-likelihood\n",
    "        l1_trace.append(l1_new)\n",
    "        if l1 is not None and (l1_new-l1) < thresh and l1_new > -np.inf:\n",
    "            l1 = l1_new\n",
    "            break\n",
    "        else:\n",
    "            l1 = l1_new\n",
    "    \n",
    "    out = {\"weights\": weights, \"means\": mu, \"covs\": sigma, \"loglik\": l1_trace, \"resp\": resp}\n",
    "    \n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = EM_for_high_dimension(tf_idf, means, covs, weights, cov_smoothing=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3879297479.366981, 4883345753.533131, 4883345753.533131]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"loglik\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret clustering results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to k-means, EM is able to explicitly model clusters of varying sizes and proportions. The relative magnitude of variances in the word dimensions tell us much about the nature of the clusters.\n",
    "\n",
    "Write yourself a cluster visualizer as follows.  Examining each cluster's mean vector, list the 5 words with the largest mean values (5 most common words in the cluster). For each word, also include the associated variance parameter (diagonal element of the covariance matrix).\n",
    "\n",
    "A sample output may be:\n",
    "```\n",
    "==========================================================\n",
    "Cluster 0: Largest mean parameters in cluster \n",
    "\n",
    "Word        Mean        Variance    \n",
    "football    1.08e-01    8.64e-03\n",
    "season      5.80e-02    2.93e-03\n",
    "club        4.48e-02    1.99e-03\n",
    "league      3.94e-02    1.08e-03\n",
    "played      3.83e-02    8.45e-04\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_EM_clusters(tf_idf, means, covs, map_index_to_word):\n",
    "    print(\"\")\n",
    "    print(\"============================================================\")\n",
    "    \n",
    "    num_clusters = len(means)\n",
    "    \n",
    "    for c in xrange(num_clusters):\n",
    "        print('Cluster {0:d}: Largest mean parameters in cluster '.format(c))\n",
    "        print('\\n{0: <12}{1: <12}{2: <12}'.format('Word', 'Mean', 'Variance'))\n",
    "        \n",
    "        # The k'th element of sorted_word_ids should be the index of the word \n",
    "        # that has the k'th-largest value in the cluster mean. Hint: Use np.argsort().\n",
    "        sorted_word_ids = np.argsort(-means[c])\n",
    "\n",
    "        for i in sorted_word_ids[:5]:\n",
    "            print '{0: <12}{1:<10.2e}{2:10.2e}'.format(map_index_to_word['category'][i], \n",
    "                                                       means[c][i],\n",
    "                                                       covs[c][i])\n",
    "        print '\\n=========================================================='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Cluster 0: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "poetry      1.51e-01    1.90e-02\n",
      "poems       6.33e-02    6.45e-03\n",
      "poet        5.91e-02    6.36e-03\n",
      "de          4.77e-02    8.72e-03\n",
      "literary    4.68e-02    3.29e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 1: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         1.60e-01    4.59e-03\n",
      "her         1.04e-01    3.20e-03\n",
      "music       1.53e-02    1.04e-03\n",
      "actress     1.52e-02    1.14e-03\n",
      "show        1.27e-02    7.33e-04\n",
      "\n",
      "==========================================================\n",
      "Cluster 2: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "football    7.45e-02    4.57e-03\n",
      "club        5.84e-02    2.55e-03\n",
      "league      5.72e-02    2.83e-03\n",
      "season      5.06e-02    2.35e-03\n",
      "played      3.79e-02    9.46e-04\n",
      "\n",
      "==========================================================\n",
      "Cluster 3: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "district    5.56e-02    4.00e-03\n",
      "republican  5.47e-02    4.55e-03\n",
      "senate      5.04e-02    5.21e-03\n",
      "democratic  3.72e-02    2.46e-03\n",
      "house       3.65e-02    2.07e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 4: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "novel       5.26e-02    5.50e-03\n",
      "book        4.30e-02    2.38e-03\n",
      "published   3.87e-02    1.51e-03\n",
      "books       3.19e-02    1.57e-03\n",
      "fiction     3.06e-02    3.10e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 5: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "rugby       1.59e-01    1.14e-02\n",
      "against     5.11e-02    1.99e-03\n",
      "wales       4.96e-02    6.96e-03\n",
      "played      4.90e-02    1.42e-03\n",
      "cup         4.81e-02    2.30e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 6: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "tour        5.18e-02    1.08e-02\n",
      "pga         4.42e-02    1.36e-02\n",
      "championship4.31e-02    3.78e-03\n",
      "racing      3.53e-02    4.77e-03\n",
      "won         3.11e-02    6.93e-04\n",
      "\n",
      "==========================================================\n",
      "Cluster 7: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "film        1.78e-01    6.05e-03\n",
      "films       6.60e-02    3.33e-03\n",
      "festival    4.46e-02    3.60e-03\n",
      "feature     3.41e-02    1.61e-03\n",
      "directed    2.83e-02    1.95e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 8: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "jazz        1.62e-01    1.65e-02\n",
      "chess       1.09e-01    3.10e-02\n",
      "grandmaster 3.78e-02    8.97e-03\n",
      "music       3.39e-02    1.15e-03\n",
      "pianist     2.69e-02    2.30e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 9: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "engineering 2.15e-02    2.76e-03\n",
      "business    2.08e-02    1.55e-03\n",
      "technology  1.90e-02    1.46e-03\n",
      "company     1.80e-02    9.69e-04\n",
      "chairman    1.69e-02    1.51e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 10: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "art         1.37e-01    6.78e-03\n",
      "museum      6.21e-02    7.84e-03\n",
      "artist      4.17e-02    1.74e-03\n",
      "gallery     3.91e-02    3.67e-03\n",
      "work        3.23e-02    7.92e-04\n",
      "\n",
      "==========================================================\n",
      "Cluster 11: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "research    4.17e-02    2.15e-03\n",
      "university  3.85e-02    7.99e-04\n",
      "professor   3.30e-02    1.27e-03\n",
      "science     2.31e-02    2.08e-03\n",
      "studies     2.11e-02    1.76e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 12: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "law         1.13e-01    8.91e-03\n",
      "court       6.40e-02    5.53e-03\n",
      "judge       4.00e-02    4.70e-03\n",
      "justice     3.63e-02    3.37e-03\n",
      "rights      3.47e-02    5.19e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 13: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "coach       9.60e-02    9.97e-03\n",
      "hockey      9.28e-02    2.04e-02\n",
      "soccer      8.30e-02    2.46e-02\n",
      "nhl         6.11e-02    1.22e-02\n",
      "season      4.47e-02    1.93e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 14: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "baseball    1.15e-01    5.53e-03\n",
      "league      1.03e-01    3.64e-03\n",
      "major       5.10e-02    1.16e-03\n",
      "games       4.71e-02    1.94e-03\n",
      "sox         4.57e-02    6.25e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 15: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "basketball  9.90e-02    1.32e-02\n",
      "football    5.98e-02    5.74e-03\n",
      "yards       5.32e-02    1.39e-02\n",
      "nba         5.21e-02    8.68e-03\n",
      "nfl         4.69e-02    7.80e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 16: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "church      1.20e-01    9.69e-03\n",
      "bishop      8.51e-02    1.06e-02\n",
      "archbishop  4.87e-02    7.44e-03\n",
      "diocese     4.70e-02    5.52e-03\n",
      "lds         4.13e-02    9.88e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 17: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "that        1.30e-02    1.97e-04\n",
      "i           1.19e-02    1.58e-03\n",
      "were        1.07e-02    2.83e-04\n",
      "he          1.03e-02    5.77e-05\n",
      "had         1.01e-02    2.24e-04\n",
      "\n",
      "==========================================================\n",
      "Cluster 18: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "music       1.12e-01    5.23e-03\n",
      "orchestra   8.40e-02    9.74e-03\n",
      "symphony    5.39e-02    8.05e-03\n",
      "conductor   4.97e-02    7.52e-03\n",
      "opera       4.84e-02    1.30e-02\n",
      "\n",
      "==========================================================\n",
      "Cluster 19: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         9.29e-02    8.21e-03\n",
      "championships8.06e-02    6.81e-03\n",
      "miss        7.95e-02    2.83e-02\n",
      "marathon    6.87e-02    2.49e-02\n",
      "metres      5.87e-02    1.08e-02\n",
      "\n",
      "==========================================================\n",
      "Cluster 20: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "minister    1.28e-01    7.34e-03\n",
      "prime       4.89e-02    4.79e-03\n",
      "government  3.65e-02    2.00e-03\n",
      "party       3.32e-02    1.33e-03\n",
      "cabinet     3.25e-02    3.34e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 21: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "news        5.64e-02    6.42e-03\n",
      "radio       4.04e-02    3.61e-03\n",
      "show        3.08e-02    2.28e-03\n",
      "television  2.40e-02    1.07e-03\n",
      "reporter    2.38e-02    1.88e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 22: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "theatre     5.66e-02    7.14e-03\n",
      "film        4.28e-02    1.66e-03\n",
      "actor       3.92e-02    3.01e-03\n",
      "television  3.67e-02    1.74e-03\n",
      "comedy      3.52e-02    4.57e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 23: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "party       6.80e-02    3.06e-03\n",
      "election    6.29e-02    3.42e-03\n",
      "elected     3.82e-02    1.24e-03\n",
      "liberal     3.56e-02    5.48e-03\n",
      "council     3.18e-02    2.33e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 24: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "album       7.41e-02    4.96e-03\n",
      "band        5.63e-02    4.27e-03\n",
      "music       4.36e-02    2.12e-03\n",
      "released    3.28e-02    1.14e-03\n",
      "records     2.34e-02    1.21e-03\n",
      "\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "visualize_EM_clusters(tf_idf, out['means'], out['covs'], map_index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1: Select all the topics that have a cluster in the model created above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing to random initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create variables for randomly initializing the EM algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "num_clusters = len(means)\n",
    "num_docs, num_words = tf_idf.shape\n",
    "\n",
    "random_means = []\n",
    "random_covs = []\n",
    "random_weights = []\n",
    "\n",
    "for k in range(num_clusters):\n",
    "    \n",
    "    # Create a numpy array of length num_words with random normally distributed values.\n",
    "    # Use the standard univariate normal distribution (mean 0, variance 1).\n",
    "    mean = np.random.normal(0, 1, num_words)\n",
    "    \n",
    "    # Create a numpy array of length num_words with random values uniformly distributed between 1 and 5.\n",
    "    cov = np.random.uniform(1,6,num_words)\n",
    "    \n",
    "    # Initially give each cluster equal weight.\n",
    "    weight = 1\n",
    "    \n",
    "    random_means.append(mean)\n",
    "    random_covs.append(cov)\n",
    "    random_weights.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_random_init = EM_for_high_dimension(tf_idf, random_means, random_covs, random_weights, cov_smoothing=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-793165403.6892343,\n",
       " 2282407852.9796767,\n",
       " 2362262754.3453236,\n",
       " 2362514453.9992857,\n",
       " 2362514453.9995394,\n",
       " 2362514453.9995394]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_random_init['loglik']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2: Try fitting EM with the random initial parameters you created above. What is the final loglikelihood that the algorithm converges to? Choose the range that contains this value.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3: Is the final loglikelihood larger or smaller than the final loglikelihood we obtained above when initializing EM with the results from running k-means?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4: For the above model, `out_random_init`, use the `visualize_EM_clusters` method you created above. Are the clusters more or less interpretable than the ones found after initializing using k-means?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Cluster 0: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         3.90e-02    5.42e-03\n",
      "her         2.54e-02    2.14e-03\n",
      "music       2.12e-02    2.34e-03\n",
      "singapore   1.77e-02    5.52e-03\n",
      "bbc         1.17e-02    1.83e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 1: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         1.63e-02    2.46e-03\n",
      "he          1.35e-02    1.09e-04\n",
      "music       1.16e-02    1.12e-03\n",
      "university  1.06e-02    3.07e-04\n",
      "her         1.03e-02    8.35e-04\n",
      "\n",
      "==========================================================\n",
      "Cluster 2: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         3.12e-02    3.56e-03\n",
      "her         2.41e-02    2.52e-03\n",
      "music       1.51e-02    1.44e-03\n",
      "he          1.10e-02    1.16e-04\n",
      "festival    1.07e-02    2.03e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 3: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         2.70e-02    3.39e-03\n",
      "her         1.81e-02    1.56e-03\n",
      "film        1.48e-02    2.16e-03\n",
      "series      1.06e-02    5.52e-04\n",
      "physics     1.05e-02    4.08e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 4: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         2.48e-02    3.28e-03\n",
      "music       1.56e-02    1.55e-03\n",
      "her         1.37e-02    1.17e-03\n",
      "he          1.23e-02    1.11e-04\n",
      "university  1.07e-02    3.15e-04\n",
      "\n",
      "==========================================================\n",
      "Cluster 5: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         2.83e-02    3.86e-03\n",
      "her         2.17e-02    2.27e-03\n",
      "league      2.02e-02    2.25e-03\n",
      "baseball    1.67e-02    2.07e-03\n",
      "season      1.45e-02    8.68e-04\n",
      "\n",
      "==========================================================\n",
      "Cluster 6: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         3.20e-02    3.80e-03\n",
      "her         2.85e-02    2.77e-03\n",
      "art         1.92e-02    4.02e-03\n",
      "museum      1.27e-02    3.25e-03\n",
      "miss        1.15e-02    4.07e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 7: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         2.41e-02    3.09e-03\n",
      "her         1.57e-02    1.38e-03\n",
      "university  1.28e-02    5.09e-04\n",
      "poker       1.16e-02    6.46e-03\n",
      "he          1.14e-02    1.08e-04\n",
      "\n",
      "==========================================================\n",
      "Cluster 8: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "league      2.66e-02    2.73e-03\n",
      "season      1.89e-02    1.31e-03\n",
      "hockey      1.69e-02    6.42e-03\n",
      "team        1.68e-02    9.65e-04\n",
      "she         1.62e-02    2.19e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 9: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         2.06e-02    2.57e-03\n",
      "her         1.44e-02    1.50e-03\n",
      "he          1.43e-02    1.31e-04\n",
      "university  1.29e-02    3.99e-04\n",
      "minister    1.11e-02    1.85e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 10: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         2.72e-02    3.26e-03\n",
      "her         2.04e-02    1.94e-03\n",
      "chess       1.84e-02    7.59e-03\n",
      "district    1.38e-02    1.77e-03\n",
      "senate      1.35e-02    2.25e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 11: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         1.88e-02    2.36e-03\n",
      "her         1.57e-02    1.92e-03\n",
      "album       1.54e-02    2.14e-03\n",
      "music       1.31e-02    9.88e-04\n",
      "he          1.30e-02    1.08e-04\n",
      "\n",
      "==========================================================\n",
      "Cluster 12: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         3.65e-02    4.97e-03\n",
      "her         2.24e-02    2.15e-03\n",
      "film        1.49e-02    2.07e-03\n",
      "he          1.16e-02    1.13e-04\n",
      "university  1.11e-02    3.66e-04\n",
      "\n",
      "==========================================================\n",
      "Cluster 13: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         2.16e-02    3.60e-03\n",
      "he          1.32e-02    9.18e-05\n",
      "university  1.16e-02    5.17e-04\n",
      "her         1.05e-02    9.54e-04\n",
      "president   9.96e-03    4.44e-04\n",
      "\n",
      "==========================================================\n",
      "Cluster 14: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "law         2.67e-02    5.77e-03\n",
      "hong        2.30e-02    7.09e-03\n",
      "kong        2.22e-02    6.38e-03\n",
      "band        1.55e-02    2.17e-03\n",
      "she         1.41e-02    2.04e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 15: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "league      1.60e-02    1.71e-03\n",
      "she         1.50e-02    1.89e-03\n",
      "he          1.33e-02    1.08e-04\n",
      "music       1.25e-02    9.96e-04\n",
      "university  1.17e-02    3.98e-04\n",
      "\n",
      "==========================================================\n",
      "Cluster 16: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "film        1.90e-02    3.37e-03\n",
      "science     1.40e-02    2.27e-03\n",
      "research    1.30e-02    1.37e-03\n",
      "music       1.24e-02    1.28e-03\n",
      "university  1.16e-02    3.15e-04\n",
      "\n",
      "==========================================================\n",
      "Cluster 17: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         3.03e-02    4.25e-03\n",
      "orchestra   1.74e-02    4.03e-03\n",
      "symphony    1.73e-02    5.01e-03\n",
      "music       1.65e-02    1.27e-03\n",
      "her         1.58e-02    1.11e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 18: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         2.18e-02    3.49e-03\n",
      "music       2.16e-02    2.67e-03\n",
      "he          1.45e-02    1.30e-04\n",
      "championship1.41e-02    2.39e-03\n",
      "her         1.29e-02    1.13e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 19: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         2.55e-02    3.81e-03\n",
      "he          1.28e-02    1.23e-04\n",
      "her         1.19e-02    1.05e-03\n",
      "served      1.14e-02    4.36e-04\n",
      "taylor      1.12e-02    3.89e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 20: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         2.17e-02    3.30e-03\n",
      "health      1.41e-02    3.91e-03\n",
      "her         1.22e-02    1.03e-03\n",
      "he          1.20e-02    9.60e-05\n",
      "member      1.02e-02    2.95e-04\n",
      "\n",
      "==========================================================\n",
      "Cluster 21: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "he          1.34e-02    9.34e-05\n",
      "she         1.30e-02    1.94e-03\n",
      "university  1.12e-02    3.16e-04\n",
      "football    1.04e-02    1.35e-03\n",
      "american    9.68e-03    4.35e-04\n",
      "\n",
      "==========================================================\n",
      "Cluster 22: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "church      1.74e-02    3.41e-03\n",
      "she         1.41e-02    2.07e-03\n",
      "he          1.36e-02    1.01e-04\n",
      "her         1.18e-02    1.41e-03\n",
      "music       1.07e-02    1.27e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 23: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         3.66e-02    6.23e-03\n",
      "her         1.96e-02    1.81e-03\n",
      "music       1.35e-02    1.28e-03\n",
      "album       1.25e-02    2.13e-03\n",
      "he          1.18e-02    1.18e-04\n",
      "\n",
      "==========================================================\n",
      "Cluster 24: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         1.74e-02    2.68e-03\n",
      "played      1.37e-02    6.68e-04\n",
      "football    1.32e-02    2.35e-03\n",
      "he          1.21e-02    8.89e-05\n",
      "season      1.11e-02    7.14e-04\n",
      "\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "visualize_EM_clusters(tf_idf, out_random_init['means'], out_random_init['covs'], map_index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Random initialization may sometimes produce a superior fit than k-means initialization. We do not claim that random initialization is always worse. However, this section does illustrate that random initialization often produces much worse clustering than k-means counterpart. This is the reason why we provide the particular random seed (`np.random.seed(5)`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
